# -*- coding: utf-8 -*-
"""190253K - Lab 01.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OlE_Go6uT1hdbpOhcCohDBu3HnRyg-fw
"""

# import libraries
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, mean_squared_error
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# read the test and train data files
train_df = pd.read_csv("train.csv")
test_df = pd.read_csv("valid.csv")
test_df_ = pd.read_csv("test.csv")

cols = train_df.columns

def to_csv(predictions_old, predictions_new, reduced_X_train, label_no):
    data = []
    cols = ["Predicted labels before feature engineering", "Predicted labels after feature engineering", "No of new features"]

    for index in range(1, 257):
        cols.append(f"new_feature_{index}")

    for index, pred in enumerate(predictions_old):
        data.append([predictions_old[index], predictions_new[index]])

    final_no_of_features = reduced_X_train.shape[1]
    for index, row in enumerate(data):
        data[index].append(final_no_of_features)
        if index < len(reduced_X_train):
            data[index] = np.concatenate((data[index], reduced_X_train[index]))

    blank_array = np.empty((1, (256 - final_no_of_features)))
    blank_array.fill(np.nan)
    for index,row in enumerate(data):
        data[index] = np.concatenate((data[index], blank_array[0]))

    data_frame = pd.DataFrame(data, columns=cols)
    data_frame.to_csv(f"190253K_{label_no}.csv",na_rep='')

"""label 01"""

train_1_df = train_df.iloc[:,:-3]
test_1_df = test_df.iloc[:, :-3]
test_1_df_ = test_df_.iloc[:, :-4]

train_1_df.dropna(inplace=True)
test_1_df.dropna(inplace=True)
test_1_df_.dropna(inplace=True)

# splitting the test and train datasets into X and Y values
X_1_train= train_1_df.iloc[:,0:-1].values
Y_1_train = train_1_df.iloc[:,-1].values
X_1_test = test_1_df.iloc[:,0:-1].values
Y_1_test = test_1_df.iloc[:,-1].values
X_1_test_ = test_1_df_.iloc[:,:].values

# scalling and fitting data
scaler = StandardScaler()
scaler.fit(X_1_train)

X_1_train = scaler.transform(X_1_train)
X_1_test = scaler.transform(X_1_test)
X_1_test_ = scaler.transform(X_1_test_)

# random forest classifier
model = RandomForestClassifier()
model.fit(X_1_train, Y_1_train)

y_1_pred = model.predict(X_1_test)
y_1_pred_ = model.predict(X_1_test_)
print(classification_report(Y_1_test, y_1_pred))

pca=PCA(0.97)
pca = pca.fit(X_1_train)

x_1_train_pca=pca.fit_transform(X_1_train)
x_1_test_pca = pca.transform(X_1_test)
x_1_test_pca_ = pca.transform(X_1_test_)

# random forest classifier
model = RandomForestClassifier()
model.fit(x_1_train_pca, Y_1_train)

y_1_pred_after = model.predict(x_1_test_pca)
print(classification_report(Y_1_test, y_1_pred_after))

# dropping the features which have low score in feature importance
importance = model.feature_importances_

columns_to_delete = []
for i,v in enumerate(importance):
    if v < 0.009:
        columns_to_delete.append(i)
x_1_train_pca_reduced = np.delete(x_1_train_pca,columns_to_delete,axis=1)
x_1_test_pca_reduced = np.delete(x_1_test_pca,columns_to_delete,axis=1)
x_1_test_pca_reduced_ = np.delete(x_1_test_pca_,columns_to_delete,axis=1)
x_1_train_pca_reduced.shape

# random forest classifier
model = RandomForestClassifier()
model.fit(x_1_train_pca_reduced, Y_1_train)

y_1_pred_after = model.predict(x_1_test_pca_reduced)
y_1_pred_after_ = model.predict(x_1_test_pca_reduced_)
print(classification_report(Y_1_test, y_1_pred_after))

to_csv(y_1_pred_, y_1_pred_after_,x_1_test_pca_reduced_, "label_1")

"""label 02"""

train_2_df = train_df.iloc[:, :-2]
test_2_df = test_df.iloc[:, :-2]
test_2_df_ = test_df_.iloc[:, :-4]

train_2_df.drop(columns=["label_1"], inplace=True)
test_2_df.drop(columns=["label_1"], inplace=True)

train_2_df.dropna(inplace=True)
test_2_df.dropna(inplace=True)
test_2_df_.dropna(inplace=True)

# splitting the test and train datasets into X and Y values
X_2_train= train_2_df.iloc[:,0:-1].values
Y_2_train = train_2_df.iloc[:,-1].values
X_2_test = test_2_df.iloc[:,0:-1].values
Y_2_test = test_2_df.iloc[:,-1].values
X_2_test_ = test_2_df_.iloc[:, :].values

# scalling and fitting data
scaler = StandardScaler()
scaler.fit(X_2_train)

X_2_train = scaler.transform(X_2_train)
X_2_test = scaler.transform(X_2_test)
X_2_test_ = scaler.transform(X_2_test_)

#install xgboost
!pip install xgboost

import xgboost as xgb

regressor = xgb.XGBRegressor()
regressor.fit(X_2_train,Y_2_train)

Y_2_pred = regressor.predict(X_2_test)
Y_2_pred_ = regressor.predict(X_2_test_)
print(f"mean squared error: {mean_squared_error(Y_2_test,Y_2_pred)}")

pca = PCA(0.8)
pca = pca.fit(X_2_train)

X_2_train_pca = pca.transform(X_2_train)
X_2_test_pca = pca.transform(X_2_test)
X_2_test_pca_ = pca.transform(X_2_test_)
X_2_train_pca.shape

# scalling and fitting data
scaler = StandardScaler()
scaler.fit(X_2_train)

X_2_train = scaler.transform(X_2_train)
X_2_test = scaler.transform(X_2_test)
X_2_test_ = scaler.transform(X_2_test_)

regressor = xgb.XGBRegressor()
regressor.fit(X_2_train_pca,Y_2_train)

Y_2_pred_after = regressor.predict(X_2_test_pca)
Y_2_pred_after_ = regressor.predict(X_2_test_pca_)
print(f"mean squared error: {mean_squared_error(Y_2_test,Y_2_pred_after)}")

to_csv(Y_2_pred_ , Y_2_pred_after_, X_2_test_pca_, "label_2")

"""label 03"""

train_3_df = train_df.iloc[:, :-1]
test_3_df = test_df.iloc[:, :-1]
test_3_df_ = test_df_.iloc[:, :-4]

train_3_df.drop(columns=["label_1", "label_2"], inplace=True)
test_3_df.drop(columns=["label_1", "label_2"], inplace=True)

test_3_df_.dropna(inplace=True)

X_3_train= train_3_df.iloc[:,0:-1].values
Y_3_train = train_3_df.iloc[:,-1].values
X_3_test = test_3_df.iloc[:,0:-1].values
Y_3_test = test_3_df.iloc[:,-1].values
X_3_test_ = test_3_df_.iloc[:, :].values

# scalling and fitting data
scaler = StandardScaler()
scaler.fit(X_3_train)

X_3_train = scaler.transform(X_3_train)
X_3_test = scaler.transform(X_3_test)
X_3_test_ = scaler.transform(X_3_test_)

# random forest classifier
model = RandomForestClassifier()
model.fit(X_3_train, Y_3_train)

y_3_pred = model.predict(X_3_test)
y_3_pred_ = model.predict(X_3_test_)
print(classification_report(Y_3_test, y_3_pred))

train_3_df['label_3'].value_counts().plot(kind='bar',title='Imbalanced data')

# resampling the data
from imblearn.combine import SMOTETomek

resampler = SMOTETomek(random_state=0)
X_3_train, Y_3_train = resampler.fit_resample(X_3_train, Y_3_train)

pca = PCA(0.98)
pca = pca.fit(X_3_train)

X_3_train_pca = pca.transform(X_3_train)
X_3_test_pca = pca.transform(X_3_test)
X_3_test_pca_ = pca.transform(X_3_test)
X_3_train_pca.shape

classifier = RandomForestClassifier()
classifier.fit(X_3_train_pca, Y_3_train)
Y_3_pred_after = classifier.predict(X_3_test_pca)
print(classification_report(Y_3_test, Y_3_pred_after))

# dropping the features which have low score in feature importance
importance = classifier.feature_importances_

columns_to_delete = []
for i,v in enumerate(importance):
    if v < 0.008:
        columns_to_delete.append(i)
x_3_train_pca_reduced = np.delete(X_3_train_pca,columns_to_delete,axis=1)
x_3_test_pca_reduced = np.delete(X_3_test_pca,columns_to_delete,axis=1)
x_3_test_pca_reduced_ = np.delete(X_3_test_pca_,columns_to_delete,axis=1)
x_3_train_pca_reduced.shape

classifier = RandomForestClassifier()
classifier.fit(x_3_train_pca_reduced, Y_3_train)
Y_3_pred_after = classifier.predict(x_3_test_pca_reduced)
Y_3_pred_after_ = classifier.predict(x_3_test_pca_reduced_)
print(classification_report(Y_3_test, Y_3_pred_after))

to_csv(y_3_pred_, Y_3_pred_after_, x_3_test_pca_reduced_, "label_3")

"""label 04"""

train_4_df = train_df.iloc[:, :]
test_4_df = test_df.iloc[:, :]
test_4_df_ = test_df_.iloc[:, :-4]

train_4_df.drop(columns=["label_1", "label_2", "label_3"], inplace=True)
test_4_df.drop(columns=["label_1", "label_2", "label_3"], inplace=True)

test_4_df_.dropna(inplace=True)

X_4_train = train_4_df.iloc[:,0:-1].values
Y_4_train = train_4_df.iloc[:,-1].values
X_4_test = test_4_df.iloc[:,0:-1].values
Y_4_test = test_4_df.iloc[:,-1].values
X_4_test_ = test_4_df_.iloc[:,:].values

# random forest classifier
model_ = RandomForestClassifier()
model_.fit(X_4_train, Y_4_train)

y_4_pred = model_.predict(X_4_test)
y_4_pred_ = model_.predict(X_4_test_)
print(classification_report(Y_4_test, y_4_pred))

pca = PCA(0.97)
pca = pca.fit(X_4_train)

X_4_train_pca = pca.transform(X_4_train)
X_4_test_pca = pca.transform(X_4_test)
X_4_test_pca_ = pca.transform(X_4_test_)
X_4_train_pca.shape

# random forest classifier
model_ = RandomForestClassifier()
model_.fit(X_4_train_pca, Y_4_train)

y_4_pred_after = model_.predict(X_4_test_pca)
y_4_pred_after_ = model_.predict(X_4_test_pca_)
print(classification_report(Y_4_test, y_4_pred))

train_4_df['label_4'].value_counts().plot(kind='bar',title='Imbalanced Label_4')

# resampling the data
from imblearn.combine import SMOTETomek

resampler = SMOTETomek(random_state=0)
X_4_train_pca, Y_4_train = resampler.fit_resample(X_4_train_pca, Y_4_train)

# dropping the features which have low score in feature importance
importance = model_.feature_importances_

columns_to_delete = []
for i,v in enumerate(importance):
    if v < 0.015:
        columns_to_delete.append(i)
x_4_train_pca_reduced = np.delete(X_4_train_pca,columns_to_delete,axis=1)
x_4_test_pca_reduced = np.delete(X_4_test_pca,columns_to_delete,axis=1)
x_4_test_pca_reduced_ = np.delete(X_4_test_pca_,columns_to_delete,axis=1)
x_4_train_pca_reduced.shape

classifier = RandomForestClassifier()
classifier.fit(x_4_train_pca_reduced, Y_4_train)
Y_4_pred_after = classifier.predict(x_4_test_pca_reduced)
Y_4_pred_after_ = classifier.predict(x_4_test_pca_reduced)
print(classification_report(Y_4_test, Y_4_pred_after))

to_csv(y_4_pred, Y_4_pred_after , x_4_test_pca_reduced_, "label_4")

